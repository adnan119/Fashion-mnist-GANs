{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conditional_fashion-mnist.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNmCtPTuGBlcRlJ/4/CB3tB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnan119/Fashion-mnist-GANs/blob/main/conditional_fashion_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfeMyG8UUwMS"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2DTranspose #<-- transposed convolution for upsampling\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.losses import Reduction\n",
        "from tensorflow import stop_gradient\n",
        "from tensorflow import one_hot\n",
        "from tensorflow import concat\n",
        "from tensorflow import tile\n",
        "from tensorflow import stop_gradient\n",
        "from sklearn.utils import shuffle\n",
        "from imutils import build_montages\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBj_CPVNVAQY"
      },
      "source": [
        "class DCGAN:\n",
        "\t@staticmethod\n",
        "\tdef build_generator(dim, depth, channels=1, inputDim=100,\n",
        "\t\toutputDim=512):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\" and the channels dimension itself\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (dim, dim, depth)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "    # first set of FC => RELU => BN layers\n",
        "\t\tmodel.add(Dense(input_dim=inputDim, units=outputDim))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization()) # <-- batch normalization layers will be used to reduce internal covariate shift\n",
        "  \n",
        "\t\t# second set of FC => RELU => BN layers, this time preparing\n",
        "\t\t# the number of FC nodes to be reshaped into a volume\n",
        "\t\tmodel.add(Dense(dim * dim * depth))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "  \n",
        "    # reshape the output of the previous layer set, upsample +\n",
        "\t\t# apply a transposed convolution, RELU, and BN\n",
        "\t\tmodel.add(Reshape(inputShape))\n",
        "\t\tmodel.add(Conv2DTranspose(32, (5, 5), strides=(2, 2),\n",
        "\t\t\tpadding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "  \n",
        "    # apply another upsample and transposed convolution, but\n",
        "\t\t# this time output the TANH activation\n",
        "\t\tmodel.add(Conv2DTranspose(channels, (5, 5), strides=(2, 2),\n",
        "\t\t\tpadding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"tanh\"))\n",
        "\t\t# return the generator model\n",
        "\t\treturn model\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef build_discriminator(width, height, depth, alpha=0.2):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\"\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\t# first set of CONV => RELU layers\n",
        "\t\tmodel.add(Conv2D(32, (5, 5), padding=\"same\", strides=(2, 2),\n",
        "\t\t\tinput_shape=inputShape))\n",
        "\t\tmodel.add(LeakyReLU(alpha=alpha))\n",
        "\t\t# second set of CONV => RELU layers\n",
        "\t\tmodel.add(Conv2D(64, (5, 5), padding=\"same\", strides=(2, 2)))\n",
        "\t\tmodel.add(LeakyReLU(alpha=alpha))\n",
        "\t\t# first (and only) set of FC => RELU layers\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(512))\n",
        "\t\tmodel.add(LeakyReLU(alpha=alpha))\n",
        "\t\t# sigmoid layer outputting a single value\n",
        "\t\tmodel.add(Dense(1))\n",
        "\t\tmodel.add(Activation(\"sigmoid\"))\n",
        "\t\t# return the discriminator model\n",
        "\t\treturn model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5dpcf8RVG7f"
      },
      "source": [
        "from numpy import zeros\n",
        "from numpy.random import randn\n",
        "from matplotlib import pyplot\n",
        "\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        " \n",
        "\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "  # generate points in latent space\n",
        "  x_input = generate_latent_points(latent_dim, n_samples)\n",
        "  # predict outputs\n",
        "  X = g_model.predict(x_input)\n",
        "  images = ((X * 127.5) + 127.5).astype(\"uint8\")\n",
        "  images = np.repeat(images, 3, axis=-1)\n",
        "  # create 'fake' class labels (0)\n",
        "  y = zeros((n_samples, 1))\n",
        "  return X, y\n",
        " "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A1vp9TIdyvU"
      },
      "source": [
        "# **Part - 2 Conditional GANs**\n",
        "# Adding Conditional Generation functionality to our GAN architecture\n",
        "\n",
        "The GAN architecture that we created above is an example of unconditional Generations i.e. the entire architecture is generated without passing any information related to the class labels.\n",
        "\n",
        "As you might have noticed above we haven't used any labels while training either the discriminator or the generator.\n",
        "\n",
        "With unconditional Generation we get output from random classes, on the other hand **conditional generation** allows us to ask for an example from a specific class and you get what you ask for, i.e. a random example from a class you specify.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In conditional GANs, the input vector for the generator will need to include the class information. The class is represented using a one-hot encoded vector where its length is the number of classes and each index represents a class. The vector is all 0's and a 1 on the chosen class.\n",
        "We'll combine/concatenate this one-hot vector with the random noise to create a new vector to be passed into the generator network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw2ScvEJWxYi"
      },
      "source": [
        "def get_one_hot_labels(labels, n_classes):\n",
        "    return one_hot(labels, n_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXaRfGaddtX0"
      },
      "source": [
        "Next, we need to be able to concatenate the one-hot class vector to the noise vector before giving it to the generator. We will also need to do this when adding the class channels to the discriminator.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef2fd6a4T1C-"
      },
      "source": [
        "def combine_vectors(x, y, dim=1):\n",
        "\n",
        "    combined = concat((x,y),dim)\n",
        "    \n",
        "    return combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMxaDqQ6ePtv"
      },
      "source": [
        "fmnist_shape = (1, 28, 28)\n",
        "n_classes = 10\n",
        "input_dim = 7\n",
        "\n",
        "#initializing a few hyper-parameters\n",
        "NUM_EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "INIT_LR = 2e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EuNBmpBgAdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80ab1d2-63cd-4880-cdc4-906bc855635a"
      },
      "source": [
        "# load the Fashion MNIST dataset and stack the training and testing\n",
        "# data points so we have additional training data\n",
        "\n",
        "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
        "trainImages = np.concatenate([trainX, testX])\n",
        "\n",
        "labels = np.concatenate([trainY, testY])\n",
        "# add in an extra dimension for the channel and scale the images\n",
        "# into the range [-1, 1] (which is the range of the tanh\n",
        "# function)\n",
        "trainImages = np.expand_dims(trainImages, axis=1)\n",
        "trainImages = (trainImages.astype(\"float\") - 127.5) / 127.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEBPtExX5hhb",
        "outputId": "a33fe78e-cfb8-427e-9a42-d06e99eae449"
      },
      "source": [
        "trainImages.shape[-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1yjsALFMrFM"
      },
      "source": [
        "Now we'll initialize our new generator and discriminator architecture, the main changes that we'd need make is mostly regarding the inputs to both the architectures.\n",
        "\n",
        "###Generator Input\n",
        "\n",
        "To pass the class information to the generator we'll pass the noise vector & concatenate the one-hot labels, the one-hot labels here will have one for the corresponding label and zeros elsewhere.\n",
        "\n",
        "So now the dimension of the new input to the generator will be: **length of the noise vector + number of classes** \n",
        "\n",
        "###Discriminator Input \n",
        "Now to pass the class information to the Dicriminator we'll create one-hot matrices which we'll concatenate along with the channel(s) of the input images. This input will then be used to train the discriminator/classifier to classify images based on not just real and fake but also based on their class labels provided by the one-hot matrices.\n",
        "\n",
        "Input dimnesion to the discriminator will be: **number of image channels + number of classes**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLEycL9GgGkW"
      },
      "source": [
        "def get_input_dimensions(input_dim, fmnist_shape, n_classes):\n",
        "\n",
        "    generator_input_dim = input_dim + n_classes\n",
        "    discriminator_im_chan = n_classes+fmnist_shape[0]\n",
        "\n",
        "    return generator_input_dim, discriminator_im_chan\n",
        "\n",
        "\n",
        "generator_input_dim, discriminator_im_chan = get_input_dimensions(input_dim, fmnist_shape, n_classes)\n",
        "\n",
        "model = DCGAN()\n",
        "conditional_generator = model.build_generator(7,64, channels=1, inputDim=generator_input_dim)\n",
        "conditional_discriminator = model.build_discriminator(28,28,discriminator_im_chan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA9qFnoKLaJG",
        "outputId": "ac4977a1-0851-4cb8-c9bf-fe4961949d57"
      },
      "source": [
        "#initializing the discriminator optimizer & loss function\n",
        "discOpt = Adam(lr=INIT_LR, beta_1=0.5, decay=INIT_LR / NUM_EPOCHS)\n",
        "conditional_discriminator.compile(loss=BinaryCrossentropy(from_logits=True,\n",
        "    reduction=Reduction.NONE), optimizer=discOpt)\n",
        "\n",
        "# build the adversarial model by first setting the discriminator to\n",
        "# *not* be trainable, then combine the generator and discriminator\n",
        "# together\n",
        "print(\"[INFO] building GAN...\")\n",
        "# conditional_discriminator.trainable = False\n",
        "# ganInput = Input(shape=(generator_input_dim,))\n",
        "# ganOutput = conditional_discriminator(conditional_generator(ganInput))\n",
        "# gan = Model(ganInput, ganOutput)\n",
        "# compile the GAN\n",
        "ganOpt = Adam(lr=INIT_LR, beta_1=0.5, decay=INIT_LR / NUM_EPOCHS)\n",
        "conditional_generator.compile(loss=BinaryCrossentropy(from_logits=True, reduction=Reduction.NONE), optimizer=discOpt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] building GAN...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_ulJSUXhQvo"
      },
      "source": [
        "import tensorflow as tf\n",
        "def update_gan(gan_output,labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    disc_input = combine_vectors(gan_output,image_one_hot_labels, dim=-1)\n",
        "    disc_output = conditional_discriminator.predict(disc_input)\n",
        "    gen_loss = bce(disc_output,tf.ones_like(disc_output))\n",
        "\n",
        "    var_list = conditional_generator.trainable_weights\n",
        "    gan_gradients = tape.gradient(gen_loss, var_list)\n",
        "\n",
        "    discOpt.apply_gradients(zip(gan_gradients,var_list))\n",
        "  # discOpt.minimize(gen_loss, var_list = conditional_generator.trainable_weights)\n",
        "  # discOpt.apply_gradients(zip(gan_gradients, conditional_generator.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVsX3-ORfnjo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "1f660b53-c47a-44aa-cd18-f3acb873d668"
      },
      "source": [
        "# loop over the epochs\n",
        "for epoch in range(0, NUM_EPOCHS):\n",
        "  # show epoch information and compute the number of batches per\n",
        "  # epoch\n",
        "  print(\"[INFO] starting epoch {} of {}...\".format(epoch + 1,\n",
        "    NUM_EPOCHS))\n",
        "  batchesPerEpoch = int(trainImages.shape[0] / BATCH_SIZE)\n",
        "  # loop over the batches\n",
        "  for i in range(0, batchesPerEpoch):\n",
        "\n",
        "    labelsBatch = labels[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
        "    imageBatch = trainImages[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
        "    # initialize an (empty) output path\n",
        "    one_hot_labels = get_one_hot_labels(labelsBatch, n_classes)\n",
        "    image_one_hot_labels = one_hot_labels[:, None, None, :]\n",
        "    image_one_hot_labels = tile(image_one_hot_labels, [1, fmnist_shape[1], fmnist_shape[2], 1])\n",
        "\n",
        "    # noise for the generator to predict on\n",
        "    noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, input_dim))\n",
        "\n",
        "    # Now we can get the images from the generator\n",
        "    # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
        "    #        2) Generate the conditioned fake images\n",
        "       \n",
        "    noise_and_labels = combine_vectors(noise, one_hot_labels)\n",
        "    fakeBatch = conditional_generator.predict(noise_and_labels)\n",
        "    print(fakeBatch.shape)\n",
        "    print(imageBatch.shape)\n",
        "\n",
        "    \n",
        "    imageBatch = np.moveaxis(imageBatch, 1, 3)\n",
        "    # print(fakeBatch.shape)\n",
        "    fake_and_label_channels = combine_vectors(fakeBatch, image_one_hot_labels, dim=-1)\n",
        "    # fake_and_label_channels = np.moveaxis(fake_and_label_channels, 1, 3)\n",
        "    real_and_label_channels = combine_vectors(imageBatch, image_one_hot_labels, dim=-1)\n",
        "    # real_and_label_channels = np.moveaxis(real_and_label_channels, 1, 3)\n",
        "    #print(noise_and_labels.shape)\n",
        "    print(fake_and_label_channels.shape)\n",
        "    print(real_and_label_channels.shape)\n",
        "    print(type(real_and_label_channels))\n",
        "\n",
        "    mix_images_batch = np.concatenate((real_and_label_channels,fake_and_label_channels),-1)\n",
        "\n",
        "    fake_labels = tf.zeros_like(fake_and_label_channels)\n",
        "    real_labels = tf.ones_like(real_and_label_channels)\n",
        "    \n",
        "    mix_labels_batch = real_labels + fake_labels\n",
        "    mix_labels_batch = np.reshape(mix_labels_batch, (-1,))\n",
        "    (mix_images_batch, mix_labels_batch) = shuffle(mix_images_batch, mix_labels_batch)\n",
        "    discLoss = conditional_discriminator.train_on_batch(mix_images_batch, mix_labels_batch)\n",
        "    #print(fake.shape)\n",
        "    #print(noise_and_labels.shape)\n",
        "\n",
        "    # Now you can get the predictions from the discriminator\n",
        "    # Steps: 1) Create the input for the discriminator\n",
        "    #           a) Combine the fake images with image_one_hot_labels, \n",
        "    #              remember to detach the generator (.detach()) so you do not backpropagate through it\n",
        "    #           b) Combine the real images with image_one_hot_labels\n",
        "    #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n",
        "    #        3) Get the discriminator's prediction on the reals as disc_real_pred\n",
        "    gen_noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, input_dim))\n",
        "    gen_noise_and_labels = combine_vectors(gen_noise, one_hot_labels)\n",
        "    # print(imageBatch.shape)\n",
        "    gan_output = conditional_generator.predict(gen_noise_and_labels)\n",
        "    update_gan(gan_output,labelsBatch)\n",
        "\n",
        "    # check to see if this is the end of an epoch, and if so,\n",
        "    # initialize the output path\n",
        "    if i == batchesPerEpoch - 1:\n",
        "      # print(\"[INFO] Step {}_{}: discriminator_loss={:.6f}, \"\n",
        "      # \"adversarial_loss={:.6f}\".format(epoch + 1, i,\n",
        "      # discLoss, ganLoss))\n",
        "      # size of the latent space\n",
        "      latent_dim = generator_input_dim\n",
        "\n",
        "      # generate samples\n",
        "      n_samples = 25\n",
        "      X, _ = generate_fake_samples(conditional_generator, latent_dim, n_samples)\n",
        "      # plot the generated samples\n",
        "      for i in range(n_samples):\n",
        "        # define subplot\n",
        "        pyplot.subplot(5, 5, 1 + i)\n",
        "        # turn off axis labels\n",
        "        pyplot.axis('off')\n",
        "        # plot single image\n",
        "        pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
        "      # show the figure\n",
        "      pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] starting epoch 1 of 50...\n",
            "(128, 28, 28, 1)\n",
            "(128, 1, 28, 28)\n",
            "(128, 28, 28, 11)\n",
            "(128, 28, 28, 11)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-b7148a4ab818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmix_labels_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_labels\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mmix_labels_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmix_labels_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mmix_images_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmix_labels_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmix_images_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmix_labels_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mdiscLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconditional_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmix_images_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmix_labels_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#print(fake.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \"\"\"\n\u001b[1;32m    700\u001b[0m     \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    582\u001b[0m                                                     n_samples))\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [128, 1103872]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_x94C0jUG1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878758d5-88ff-4522-8e07-26c6738b2a92"
      },
      "source": [
        "repeat([[1, 2], [3, 4]], repeats=[2, 3], axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 5), dtype=int32, numpy=\n",
              "array([[1, 1, 2, 2, 2],\n",
              "       [3, 3, 4, 4, 4]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOr876tceKgm",
        "outputId": "6357e8f0-a58a-4e75-b5c0-443e1d3f602a"
      },
      "source": [
        "import torch\n",
        "import tensorflow\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "t1 = [[1, 2, 3], [4, 5, 6]]\n",
        "t2 = [[7, 8, 9], [10, 11, 12]]\n",
        "print(concat([t1, t2], 1))\n",
        "t1 = torch.Tensor(t1)\n",
        "t2 = torch.Tensor(t2)\n",
        "print(torch.cat((t1,t2),1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 1  2  3  7  8  9]\n",
            " [ 4  5  6 10 11 12]], shape=(2, 6), dtype=int32)\n",
            "tensor([[ 1.,  2.,  3.,  7.,  8.,  9.],\n",
            "        [ 4.,  5.,  6., 10., 11., 12.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcO0kJfoHGll",
        "outputId": "53e20fb7-167e-4c08-9cff-ede229093f25"
      },
      "source": [
        "x = torch.Tensor([[1,2],[3,4]])\n",
        "y = tensorflow.constant([[1,2],[3,4]])\n",
        "\n",
        "x = x.repeat(2,2)\n",
        "print(x)\n",
        "print(x.shape)\n",
        "\n",
        "y = tile(y,[2,2])\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 2., 1., 2.],\n",
            "        [3., 4., 3., 4.],\n",
            "        [1., 2., 1., 2.],\n",
            "        [3., 4., 3., 4.]])\n",
            "torch.Size([4, 4])\n",
            "tf.Tensor(\n",
            "[[1 2 1 2]\n",
            " [3 4 3 4]\n",
            " [1 2 1 2]\n",
            " [3 4 3 4]], shape=(4, 4), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HScWNhfBMOFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bfeeaf4-5f20-44fe-bbf7-30980b459978"
      },
      "source": [
        "labelsBatch = labels[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
        "# imageBatch = trainImages[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
        "# initialize an (empty) output path\n",
        "one_hot_labels = get_one_hot_labels(labelsBatch, n_classes)\n",
        "image_one_hot_labels = one_hot_labels[:, None, None, :]\n",
        "image_one_hot_labels = tile(image_one_hot_labels, [1, fmnist_shape[1], fmnist_shape[2], 1])\n",
        "print(image_one_hot_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 28, 28, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17pav7_0s6FX",
        "outputId": "254f043d-4623-4210-af4d-d66fdeebb516"
      },
      "source": [
        "one = tf.ones_like(real_and_label_channels)\n",
        "zero = tf.zeros_like(fake_and_label_channels)\n",
        "mix = np.concatenate((one,zero),-1)\n",
        "print(one.shape)\n",
        "print(zero.shape)\n",
        "print(mix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 28, 28, 11)\n",
            "(128, 28, 28, 11)\n",
            "(128, 28, 28, 22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h87j6AEcs_B7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}